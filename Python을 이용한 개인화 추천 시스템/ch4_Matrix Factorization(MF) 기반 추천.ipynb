{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68a10855",
   "metadata": {},
   "source": [
    "# 4.3 SGD를 사용한 MF 기본 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbff6497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.chdir('C:/RecSys/Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ea239ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating\n",
       "0      196       242       3\n",
       "1      186       302       3\n",
       "2       22       377       1\n",
       "3      244        51       2\n",
       "4      166       346       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_csv('u.data', names=r_cols,  sep='\\t',encoding='latin-1')\n",
    "ratings = ratings[['user_id', 'movie_id', 'rating']].astype(int)\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3c2f52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF():\n",
    "    def __init__(self, ratings, K, alpha, beta, iterations, verbose=True):\n",
    "        # rating_matrix\n",
    "        self.R = np.array(ratings)\n",
    "        # user, item 수 ← rating_matrix의 행, 열\n",
    "        self.num_users, self.num_items = np.shape(self.R)\n",
    "        \n",
    "        # 잠재요인(latent factor) 수\n",
    "        self.K = K\n",
    "        # 학습률\n",
    "        self.alpha = alpha\n",
    "        # 정규화 계수\n",
    "        self.beta = beta\n",
    "        # 반복 횟수\n",
    "        self.iterations = iterations\n",
    "        # SGD 중간 학습과정 출력 여부\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def rmse(self):\n",
    "        # Rating Matrix에서 평점이 있는 요소의 인덱스만 가져옴\n",
    "        xs, ys = self.R.nonzero()\n",
    "        self.predictions = []\n",
    "        self.errors = []\n",
    "        \n",
    "        for x, y in zip(xs, ys):\n",
    "            prediction = self.get_prediction(x, y)\n",
    "            self.predictions.append(prediction)\n",
    "            self.errors.append(self.R[x,y] - prediction)\n",
    "        # r hat\n",
    "        self.predictions = np.array(self.predictions)\n",
    "        \n",
    "        # 오차 계산\n",
    "        self.errors = np.array(self.errors)\n",
    "        \n",
    "        return np.sqrt(np.mean(self.errors)**2)\n",
    "    \n",
    "    def train(self):\n",
    "        # 평균이 0이고 표준편차가 1/k인 정규분포 난수 생성\n",
    "        self.P = np.random.normal(0, 1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(0, 1./self.K, size=(self.num_items, self.K))\n",
    "\n",
    "        # 사용자 평가경향 0으로 초기화 → 크기는 user의 수\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        # 아이템 평가경향 0으로 초기화 → 크기는 item의 수\n",
    "        self.b_d = np.zeros(self.num_items)\n",
    "        # rating 전체 평균 구하기\n",
    "        self.b = np.mean(self.R[self.R.nonzero()])\n",
    "\n",
    "\n",
    "        # SGD를 적용할 평점이 있는 요소의 인덱스 및 평점을 가져와서 리스트로 만듦\n",
    "        rows, columns = self.R.nonzero()\n",
    "        self.samples = [(i, j, self.R[i, j]) for i, j in zip(rows, columns)]\n",
    "\n",
    "        # SGD RMSE 히스토리 반영\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            # 어디서 시작하느냐에 따라서 수렴 속도가 달라질 수 있기 때문\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            rmse = self.rmse()\n",
    "            training_process.append((i+1, rmse))\n",
    "\n",
    "            # 중간결과 10번마다 출력\n",
    "            if self.verbose:\n",
    "                if (i+1)%10 == 0:\n",
    "                    print(\"Iteration : %d; Train RMSE = %.4f\" %(i+1, rmse))\n",
    "\n",
    "        return training_process\n",
    "    \n",
    "    def get_prediction(self, i, j):\n",
    "        prediction = self.b + self.b_u[i] + self.b_d[j] + np.dot(self.P[i, :], self.Q[j,:].T)\n",
    "        return prediction\n",
    "    \n",
    "    def sgd(self):\n",
    "        for i, j, r in self.samples:\n",
    "            # 사용자 i, 아이템 j에 대한 평점 예측치\n",
    "            prediction = self.get_prediction(i, j)\n",
    "            \n",
    "            # 오차 구하기\n",
    "            e = (r - prediction)\n",
    "            \n",
    "            # update\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_d[j] += self.alpha * (e - self.beta * self.b_d[j])\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * self.P[i, :] - self.beta * self.Q[j,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dc3306c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 10; Train RMSE = 0.0236\n",
      "Iteration : 20; Train RMSE = 0.0164\n",
      "Iteration : 30; Train RMSE = 0.0125\n",
      "Iteration : 40; Train RMSE = 0.0102\n",
      "Iteration : 50; Train RMSE = 0.0086\n",
      "Iteration : 60; Train RMSE = 0.0070\n",
      "Iteration : 70; Train RMSE = 0.0063\n",
      "Iteration : 80; Train RMSE = 0.0053\n",
      "Iteration : 90; Train RMSE = 0.0050\n",
      "Iteration : 100; Train RMSE = 0.0048\n"
     ]
    }
   ],
   "source": [
    "R_temp = ratings.pivot(index='user_id', columns='movie_id', values='rating').fillna(0)\n",
    "mf = MF(R_temp, K=30, alpha=0.001, beta=0.02, iterations=100, verbose=True)\n",
    "train_process = mf.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa5090e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f0c6ef",
   "metadata": {},
   "source": [
    "# 4.4 train_test 분리 MF 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6dc5778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "TRAIN_SIZE = 0.75\n",
    "\n",
    "ratings = shuffle(ratings, random_state=1)\n",
    "\n",
    "# train_data 갯수 → cutoff\n",
    "cutoff = int(TRAIN_SIZE * len(ratings))\n",
    "\n",
    "# train_test_split\n",
    "ratings_train = ratings.iloc[:cutoff]\n",
    "ratings_test = ratings.loc[cutoff:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd959465",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9783b",
   "metadata": {},
   "source": [
    "## 연습문제 4-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d0b5ba",
   "metadata": {},
   "source": [
    "shuffle없이 train_test_split 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c396d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ratings_train, ratings_test = train_test_split(ratings, test_size=0.25)\n",
    "# ratings_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b72c2d8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c0dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NEW_MF():\n",
    "    def __init__(self, ratings, K, alpha, beta, iterations, verbose=True):\n",
    "        self.R = np.array(ratings)\n",
    "        \n",
    "        item_id_index = []\n",
    "        index_item_id = []\n",
    "        \n",
    "        for i, one_id in enumerate(ratings):\n",
    "            item_id_index.append([one_id, i])\n",
    "            index_item_id.append([i, one_id])\n",
    "            \n",
    "        self.item_id_index = dict(item_id_index)\n",
    "        self.index_item_id = dict(index_item_id)\n",
    "        \n",
    "        user_id_index = []\n",
    "        index_user_id = []\n",
    "        \n",
    "        for i, one_id in enumerate(ratings.T):\n",
    "            user_id_index.append([one_id, i])\n",
    "            index_user_id.append([i, one_id])\n",
    "            \n",
    "        self.user_id_index = dict(user_id_index)\n",
    "        self.index_user_id = dict(index_user_id)\n",
    "        \n",
    "        self.num_users, self.num_items = np.shape(self.R)\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def rmse(self):\n",
    "        xs, ys = self.R.nonzero()\n",
    "        self.predictions = []\n",
    "        self.errors = []\n",
    "        for x, y in zip(xs, ys):\n",
    "            prediction = self.get_prediction(x, y)\n",
    "            self.predictions.append(prediction)\n",
    "            self.errors.append(self.R[x, y] - prediction)\n",
    "        self.predictions = np.array(self.predictions)\n",
    "        self.errors = np.array(self.errors)\n",
    "        return np.sqrt(np.mean(self.errors**2))\n",
    "        \n",
    "    def get_prediction(self, i, j):\n",
    "        prediction = self.b + self.b_u[i] + self.b_d[j] + np.dot(self.P[i,:], self.Q[j,:].T)\n",
    "        return prediction\n",
    "        \n",
    "    def sgd(self):\n",
    "        for i, j, r in self.samples:\n",
    "            prediction = self.get_prediction(i, j)\n",
    "            e = (r - prediction)\n",
    "\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_d[j] += self.alpha * (e - self.beta * self.b_d[j])\n",
    "\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * self.P[i, :] - self.beta * self.Q[j,:])\n",
    "\n",
    "    def set_test(self, ratings_test):\n",
    "        test_set = []\n",
    "        for i in range(len(ratings_test)):\n",
    "            # user_index 가져오기\n",
    "            x = self.user_id_index[ratings_test.iloc[i, 0]]\n",
    "            # item_index 가져오기\n",
    "            y = self.item_id_index[ratings_test.iloc[i, 1]]\n",
    "            # 해당 평점 가져오기\n",
    "            z = ratings_test.iloc[i, 2]\n",
    "            test_set.append([x, y, z])\n",
    "            \n",
    "            # 해당 평점 0으로 초기화 → train에서 제외\n",
    "            self.R[x, y] = 0\n",
    "        self.test_set = test_set\n",
    "        return test_set\n",
    "    \n",
    "    def test_rmse(self):\n",
    "        error = 0 \n",
    "        for one_set in self.test_set:\n",
    "            predicted = self.get_prediction(one_set[0], one_set[1])\n",
    "            error += pow(one_set[2] - predicted, 2)\n",
    "        return np.sqrt(error/len(self.test_set))\n",
    "    \n",
    "    def test(self):\n",
    "        self.P = np.random.normal(0, scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(0, scale=1./self.K, size=(self.num_items, self.K))\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_d = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[self.R.nonzero()])\n",
    "        rows, columns = self.R.nonzero()\n",
    "        self.samples = [(i, j, self.R[i,j]) for i, j in zip(rows, columns)]\n",
    "        \n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            rmse1 = self.rmse()\n",
    "            rmse2 = self.test_rmse()\n",
    "            training_process.append((i+1, rmse1, rmse2))\n",
    "            if self.verbose:\n",
    "                if (i+1) % 10 == 0:\n",
    "                    print(\"Iteration : %d ; Train RMSE = %.4f ; Test RMSE = %.4f\" %(i+1, rmse1, rmse2))\n",
    "        return training_process\n",
    "    \n",
    "    def get_one_prediction(self, user_id, item_id):\n",
    "        return self.get_prediction(self.user_id_index[user_id], self.item_id_index[item_id])\n",
    "    \n",
    "    def full_prediction(self):\n",
    "        return self.b + self.b_u[:,np.newaxis] + self.b_d[np.newaxis,:] + self.P.dot(self.Q.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f1bbc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 10 ; Train RMSE = 1.0357 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9911 ; Test RMSE = 1.0273\n",
      "Iteration : 30 ; Train RMSE = 0.9638 ; Test RMSE = 1.0117\n",
      "Iteration : 40 ; Train RMSE = 0.9452 ; Test RMSE = 1.0026\n",
      "Iteration : 50 ; Train RMSE = 0.9313 ; Test RMSE = 0.9969\n",
      "Iteration : 60 ; Train RMSE = 0.9205 ; Test RMSE = 0.9932\n",
      "Iteration : 70 ; Train RMSE = 0.9116 ; Test RMSE = 0.9906\n",
      "Iteration : 80 ; Train RMSE = 0.9041 ; Test RMSE = 0.9887\n",
      "Iteration : 90 ; Train RMSE = 0.8976 ; Test RMSE = 0.9874\n",
      "Iteration : 100 ; Train RMSE = 0.8917 ; Test RMSE = 0.9865\n"
     ]
    }
   ],
   "source": [
    "R_temp = ratings.pivot(index='user_id', columns='movie_id', values='rating').fillna(0)\n",
    "mf = NEW_MF(R_temp, K=30, alpha=0.001, beta=0.02, iterations=100, verbose=True)\n",
    "test_set = mf.set_test(ratings_test)\n",
    "result = mf.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a8184e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.77912065 3.2357575  3.40996533 ... 3.31983506 3.32266977 3.32033941]\n",
      " [4.23631092 3.70482578 3.87802002 ... 3.78881624 3.79917813 3.79725443]\n",
      " [3.71974389 3.18785704 3.36292696 ... 3.27968291 3.26937953 3.27758298]\n",
      " ...\n",
      " [4.09105511 3.57561337 3.74930153 ... 3.65197818 3.64533562 3.65452599]\n",
      " [4.53350191 3.9964691  4.16974764 ... 4.09787888 4.09790829 4.09852168]\n",
      " [3.68992787 3.18941869 3.35199121 ... 3.26551179 3.24985703 3.24861239]]\n",
      "3.2357575023786644\n"
     ]
    }
   ],
   "source": [
    "print(mf.full_prediction())\n",
    "print(mf.get_one_prediction(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d9d1f6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a1583",
   "metadata": {},
   "source": [
    "# 4.5 MF의 최적 파라미터 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e537f5cd",
   "metadata": {},
   "source": [
    "잠재요인의 수(K)에 따라, iterations의 수에 따라 정확도가 달라지므로 여러 번의 실험으로 최적의 정확도 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0a240c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K =  50\n",
      "Iteration : 10 ; Train RMSE = 1.0359 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9915 ; Test RMSE = 1.0273\n",
      "Iteration : 30 ; Train RMSE = 0.9646 ; Test RMSE = 1.0117\n",
      "Iteration : 40 ; Train RMSE = 0.9461 ; Test RMSE = 1.0026\n",
      "Iteration : 50 ; Train RMSE = 0.9325 ; Test RMSE = 0.9969\n",
      "Iteration : 60 ; Train RMSE = 0.9220 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9134 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9062 ; Test RMSE = 0.9887\n",
      "Iteration : 90 ; Train RMSE = 0.9000 ; Test RMSE = 0.9874\n",
      "Iteration : 100 ; Train RMSE = 0.8945 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8897 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8852 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8810 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8771 ; Test RMSE = 0.9847\n",
      "Iteration : 150 ; Train RMSE = 0.8733 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8696 ; Test RMSE = 0.9845\n",
      "Iteration : 170 ; Train RMSE = 0.8659 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8621 ; Test RMSE = 0.9845\n",
      "Iteration : 190 ; Train RMSE = 0.8582 ; Test RMSE = 0.9846\n",
      "Iteration : 200 ; Train RMSE = 0.8542 ; Test RMSE = 0.9847\n",
      "Iteration : 210 ; Train RMSE = 0.8499 ; Test RMSE = 0.9849\n",
      "Iteration : 220 ; Train RMSE = 0.8453 ; Test RMSE = 0.9850\n",
      "Iteration : 230 ; Train RMSE = 0.8404 ; Test RMSE = 0.9852\n",
      "Iteration : 240 ; Train RMSE = 0.8350 ; Test RMSE = 0.9854\n",
      "Iteration : 250 ; Train RMSE = 0.8293 ; Test RMSE = 0.9856\n",
      "Iteration : 260 ; Train RMSE = 0.8230 ; Test RMSE = 0.9859\n",
      "Iteration : 270 ; Train RMSE = 0.8161 ; Test RMSE = 0.9861\n",
      "Iteration : 280 ; Train RMSE = 0.8087 ; Test RMSE = 0.9863\n",
      "Iteration : 290 ; Train RMSE = 0.8007 ; Test RMSE = 0.9866\n",
      "Iteration : 300 ; Train RMSE = 0.7920 ; Test RMSE = 0.9869\n",
      "K =  60\n",
      "Iteration : 10 ; Train RMSE = 1.0360 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9917 ; Test RMSE = 1.0273\n",
      "Iteration : 30 ; Train RMSE = 0.9648 ; Test RMSE = 1.0117\n",
      "Iteration : 40 ; Train RMSE = 0.9464 ; Test RMSE = 1.0026\n",
      "Iteration : 50 ; Train RMSE = 0.9329 ; Test RMSE = 0.9969\n",
      "Iteration : 60 ; Train RMSE = 0.9224 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9139 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9067 ; Test RMSE = 0.9886\n",
      "Iteration : 90 ; Train RMSE = 0.9006 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8953 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8905 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8861 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8821 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8783 ; Test RMSE = 0.9846\n",
      "Iteration : 150 ; Train RMSE = 0.8746 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8711 ; Test RMSE = 0.9844\n",
      "Iteration : 170 ; Train RMSE = 0.8675 ; Test RMSE = 0.9844\n",
      "Iteration : 180 ; Train RMSE = 0.8640 ; Test RMSE = 0.9844\n",
      "Iteration : 190 ; Train RMSE = 0.8603 ; Test RMSE = 0.9845\n",
      "Iteration : 200 ; Train RMSE = 0.8565 ; Test RMSE = 0.9846\n",
      "Iteration : 210 ; Train RMSE = 0.8525 ; Test RMSE = 0.9847\n",
      "Iteration : 220 ; Train RMSE = 0.8482 ; Test RMSE = 0.9849\n",
      "Iteration : 230 ; Train RMSE = 0.8437 ; Test RMSE = 0.9851\n",
      "Iteration : 240 ; Train RMSE = 0.8387 ; Test RMSE = 0.9852\n",
      "Iteration : 250 ; Train RMSE = 0.8333 ; Test RMSE = 0.9854\n",
      "Iteration : 260 ; Train RMSE = 0.8274 ; Test RMSE = 0.9856\n",
      "Iteration : 270 ; Train RMSE = 0.8210 ; Test RMSE = 0.9858\n",
      "Iteration : 280 ; Train RMSE = 0.8141 ; Test RMSE = 0.9860\n",
      "Iteration : 290 ; Train RMSE = 0.8065 ; Test RMSE = 0.9862\n",
      "Iteration : 300 ; Train RMSE = 0.7983 ; Test RMSE = 0.9864\n",
      "K =  70\n",
      "Iteration : 10 ; Train RMSE = 1.0360 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9918 ; Test RMSE = 1.0272\n",
      "Iteration : 30 ; Train RMSE = 0.9649 ; Test RMSE = 1.0117\n",
      "Iteration : 40 ; Train RMSE = 0.9466 ; Test RMSE = 1.0026\n",
      "Iteration : 50 ; Train RMSE = 0.9331 ; Test RMSE = 0.9969\n",
      "Iteration : 60 ; Train RMSE = 0.9226 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9142 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9071 ; Test RMSE = 0.9887\n",
      "Iteration : 90 ; Train RMSE = 0.9011 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8958 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8911 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8868 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8829 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8792 ; Test RMSE = 0.9847\n",
      "Iteration : 150 ; Train RMSE = 0.8757 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8723 ; Test RMSE = 0.9845\n",
      "Iteration : 170 ; Train RMSE = 0.8690 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8657 ; Test RMSE = 0.9845\n",
      "Iteration : 190 ; Train RMSE = 0.8623 ; Test RMSE = 0.9846\n",
      "Iteration : 200 ; Train RMSE = 0.8587 ; Test RMSE = 0.9847\n",
      "Iteration : 210 ; Train RMSE = 0.8551 ; Test RMSE = 0.9848\n",
      "Iteration : 220 ; Train RMSE = 0.8512 ; Test RMSE = 0.9850\n",
      "Iteration : 230 ; Train RMSE = 0.8470 ; Test RMSE = 0.9852\n",
      "Iteration : 240 ; Train RMSE = 0.8425 ; Test RMSE = 0.9854\n",
      "Iteration : 250 ; Train RMSE = 0.8376 ; Test RMSE = 0.9856\n",
      "Iteration : 260 ; Train RMSE = 0.8322 ; Test RMSE = 0.9858\n",
      "Iteration : 270 ; Train RMSE = 0.8264 ; Test RMSE = 0.9860\n",
      "Iteration : 280 ; Train RMSE = 0.8201 ; Test RMSE = 0.9862\n",
      "Iteration : 290 ; Train RMSE = 0.8132 ; Test RMSE = 0.9864\n",
      "Iteration : 300 ; Train RMSE = 0.8058 ; Test RMSE = 0.9866\n",
      "K =  80\n",
      "Iteration : 10 ; Train RMSE = 1.0361 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9918 ; Test RMSE = 1.0272\n",
      "Iteration : 30 ; Train RMSE = 0.9650 ; Test RMSE = 1.0116\n",
      "Iteration : 40 ; Train RMSE = 0.9467 ; Test RMSE = 1.0026\n",
      "Iteration : 50 ; Train RMSE = 0.9333 ; Test RMSE = 0.9968\n",
      "Iteration : 60 ; Train RMSE = 0.9228 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9144 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9074 ; Test RMSE = 0.9886\n",
      "Iteration : 90 ; Train RMSE = 0.9014 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8962 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8915 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8873 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8835 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8799 ; Test RMSE = 0.9847\n",
      "Iteration : 150 ; Train RMSE = 0.8765 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8732 ; Test RMSE = 0.9845\n",
      "Iteration : 170 ; Train RMSE = 0.8700 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8668 ; Test RMSE = 0.9845\n",
      "Iteration : 190 ; Train RMSE = 0.8636 ; Test RMSE = 0.9846\n",
      "Iteration : 200 ; Train RMSE = 0.8603 ; Test RMSE = 0.9847\n",
      "Iteration : 210 ; Train RMSE = 0.8569 ; Test RMSE = 0.9849\n",
      "Iteration : 220 ; Train RMSE = 0.8532 ; Test RMSE = 0.9850\n",
      "Iteration : 230 ; Train RMSE = 0.8493 ; Test RMSE = 0.9852\n",
      "Iteration : 240 ; Train RMSE = 0.8452 ; Test RMSE = 0.9854\n",
      "Iteration : 250 ; Train RMSE = 0.8407 ; Test RMSE = 0.9856\n",
      "Iteration : 260 ; Train RMSE = 0.8357 ; Test RMSE = 0.9858\n",
      "Iteration : 270 ; Train RMSE = 0.8304 ; Test RMSE = 0.9861\n",
      "Iteration : 280 ; Train RMSE = 0.8245 ; Test RMSE = 0.9863\n",
      "Iteration : 290 ; Train RMSE = 0.8182 ; Test RMSE = 0.9865\n",
      "Iteration : 300 ; Train RMSE = 0.8113 ; Test RMSE = 0.9868\n",
      "K =  90\n",
      "Iteration : 10 ; Train RMSE = 1.0361 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9919 ; Test RMSE = 1.0272\n",
      "Iteration : 30 ; Train RMSE = 0.9651 ; Test RMSE = 1.0116\n",
      "Iteration : 40 ; Train RMSE = 0.9468 ; Test RMSE = 1.0025\n",
      "Iteration : 50 ; Train RMSE = 0.9334 ; Test RMSE = 0.9968\n",
      "Iteration : 60 ; Train RMSE = 0.9230 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9146 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9076 ; Test RMSE = 0.9886\n",
      "Iteration : 90 ; Train RMSE = 0.9017 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8965 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8919 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8878 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8840 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8805 ; Test RMSE = 0.9846\n",
      "Iteration : 150 ; Train RMSE = 0.8771 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8740 ; Test RMSE = 0.9845\n",
      "Iteration : 170 ; Train RMSE = 0.8709 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8678 ; Test RMSE = 0.9845\n",
      "Iteration : 190 ; Train RMSE = 0.8647 ; Test RMSE = 0.9846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 200 ; Train RMSE = 0.8616 ; Test RMSE = 0.9847\n",
      "Iteration : 210 ; Train RMSE = 0.8583 ; Test RMSE = 0.9848\n",
      "Iteration : 220 ; Train RMSE = 0.8549 ; Test RMSE = 0.9850\n",
      "Iteration : 230 ; Train RMSE = 0.8513 ; Test RMSE = 0.9852\n",
      "Iteration : 240 ; Train RMSE = 0.8474 ; Test RMSE = 0.9854\n",
      "Iteration : 250 ; Train RMSE = 0.8431 ; Test RMSE = 0.9856\n",
      "Iteration : 260 ; Train RMSE = 0.8386 ; Test RMSE = 0.9858\n",
      "Iteration : 270 ; Train RMSE = 0.8336 ; Test RMSE = 0.9860\n",
      "Iteration : 280 ; Train RMSE = 0.8281 ; Test RMSE = 0.9862\n",
      "Iteration : 290 ; Train RMSE = 0.8221 ; Test RMSE = 0.9864\n",
      "Iteration : 300 ; Train RMSE = 0.8157 ; Test RMSE = 0.9866\n",
      "K =  100\n",
      "Iteration : 10 ; Train RMSE = 1.0361 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9920 ; Test RMSE = 1.0272\n",
      "Iteration : 30 ; Train RMSE = 0.9652 ; Test RMSE = 1.0117\n",
      "Iteration : 40 ; Train RMSE = 0.9469 ; Test RMSE = 1.0026\n",
      "Iteration : 50 ; Train RMSE = 0.9335 ; Test RMSE = 0.9969\n",
      "Iteration : 60 ; Train RMSE = 0.9232 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9148 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9078 ; Test RMSE = 0.9886\n",
      "Iteration : 90 ; Train RMSE = 0.9019 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8968 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8922 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8881 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8844 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8810 ; Test RMSE = 0.9847\n",
      "Iteration : 150 ; Train RMSE = 0.8777 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8746 ; Test RMSE = 0.9845\n",
      "Iteration : 170 ; Train RMSE = 0.8717 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8687 ; Test RMSE = 0.9845\n",
      "Iteration : 190 ; Train RMSE = 0.8658 ; Test RMSE = 0.9846\n",
      "Iteration : 200 ; Train RMSE = 0.8628 ; Test RMSE = 0.9847\n",
      "Iteration : 210 ; Train RMSE = 0.8598 ; Test RMSE = 0.9849\n",
      "Iteration : 220 ; Train RMSE = 0.8566 ; Test RMSE = 0.9850\n",
      "Iteration : 230 ; Train RMSE = 0.8532 ; Test RMSE = 0.9852\n",
      "Iteration : 240 ; Train RMSE = 0.8496 ; Test RMSE = 0.9854\n",
      "Iteration : 250 ; Train RMSE = 0.8457 ; Test RMSE = 0.9856\n",
      "Iteration : 260 ; Train RMSE = 0.8414 ; Test RMSE = 0.9858\n",
      "Iteration : 270 ; Train RMSE = 0.8368 ; Test RMSE = 0.9860\n",
      "Iteration : 280 ; Train RMSE = 0.8317 ; Test RMSE = 0.9863\n",
      "Iteration : 290 ; Train RMSE = 0.8261 ; Test RMSE = 0.9865\n",
      "Iteration : 300 ; Train RMSE = 0.8200 ; Test RMSE = 0.9867\n",
      "K =  110\n",
      "Iteration : 10 ; Train RMSE = 1.0361 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9920 ; Test RMSE = 1.0273\n",
      "Iteration : 30 ; Train RMSE = 0.9652 ; Test RMSE = 1.0117\n",
      "Iteration : 40 ; Train RMSE = 0.9470 ; Test RMSE = 1.0026\n",
      "Iteration : 50 ; Train RMSE = 0.9336 ; Test RMSE = 0.9968\n",
      "Iteration : 60 ; Train RMSE = 0.9233 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9149 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9080 ; Test RMSE = 0.9887\n",
      "Iteration : 90 ; Train RMSE = 0.9021 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8970 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8925 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8884 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8847 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8813 ; Test RMSE = 0.9847\n",
      "Iteration : 150 ; Train RMSE = 0.8781 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8751 ; Test RMSE = 0.9845\n",
      "Iteration : 170 ; Train RMSE = 0.8722 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8693 ; Test RMSE = 0.9845\n",
      "Iteration : 190 ; Train RMSE = 0.8665 ; Test RMSE = 0.9846\n",
      "Iteration : 200 ; Train RMSE = 0.8636 ; Test RMSE = 0.9847\n",
      "Iteration : 210 ; Train RMSE = 0.8606 ; Test RMSE = 0.9849\n",
      "Iteration : 220 ; Train RMSE = 0.8576 ; Test RMSE = 0.9851\n",
      "Iteration : 230 ; Train RMSE = 0.8543 ; Test RMSE = 0.9852\n",
      "Iteration : 240 ; Train RMSE = 0.8508 ; Test RMSE = 0.9854\n",
      "Iteration : 250 ; Train RMSE = 0.8471 ; Test RMSE = 0.9856\n",
      "Iteration : 260 ; Train RMSE = 0.8431 ; Test RMSE = 0.9859\n",
      "Iteration : 270 ; Train RMSE = 0.8387 ; Test RMSE = 0.9861\n",
      "Iteration : 280 ; Train RMSE = 0.8339 ; Test RMSE = 0.9863\n",
      "Iteration : 290 ; Train RMSE = 0.8286 ; Test RMSE = 0.9865\n",
      "Iteration : 300 ; Train RMSE = 0.8229 ; Test RMSE = 0.9868\n",
      "K =  120\n",
      "Iteration : 10 ; Train RMSE = 1.0362 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9920 ; Test RMSE = 1.0272\n",
      "Iteration : 30 ; Train RMSE = 0.9652 ; Test RMSE = 1.0116\n",
      "Iteration : 40 ; Train RMSE = 0.9470 ; Test RMSE = 1.0026\n",
      "Iteration : 50 ; Train RMSE = 0.9337 ; Test RMSE = 0.9968\n",
      "Iteration : 60 ; Train RMSE = 0.9233 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9150 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9081 ; Test RMSE = 0.9886\n",
      "Iteration : 90 ; Train RMSE = 0.9022 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8971 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8926 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8886 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8849 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8816 ; Test RMSE = 0.9847\n",
      "Iteration : 150 ; Train RMSE = 0.8784 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8755 ; Test RMSE = 0.9845\n",
      "Iteration : 170 ; Train RMSE = 0.8726 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8698 ; Test RMSE = 0.9845\n",
      "Iteration : 190 ; Train RMSE = 0.8671 ; Test RMSE = 0.9846\n",
      "Iteration : 200 ; Train RMSE = 0.8643 ; Test RMSE = 0.9847\n",
      "Iteration : 210 ; Train RMSE = 0.8615 ; Test RMSE = 0.9849\n",
      "Iteration : 220 ; Train RMSE = 0.8586 ; Test RMSE = 0.9850\n",
      "Iteration : 230 ; Train RMSE = 0.8555 ; Test RMSE = 0.9852\n",
      "Iteration : 240 ; Train RMSE = 0.8522 ; Test RMSE = 0.9854\n",
      "Iteration : 250 ; Train RMSE = 0.8487 ; Test RMSE = 0.9856\n",
      "Iteration : 260 ; Train RMSE = 0.8449 ; Test RMSE = 0.9858\n",
      "Iteration : 270 ; Train RMSE = 0.8407 ; Test RMSE = 0.9860\n",
      "Iteration : 280 ; Train RMSE = 0.8361 ; Test RMSE = 0.9863\n",
      "Iteration : 290 ; Train RMSE = 0.8311 ; Test RMSE = 0.9865\n",
      "Iteration : 300 ; Train RMSE = 0.8256 ; Test RMSE = 0.9867\n",
      "K =  130\n",
      "Iteration : 10 ; Train RMSE = 1.0362 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9920 ; Test RMSE = 1.0272\n",
      "Iteration : 30 ; Train RMSE = 0.9653 ; Test RMSE = 1.0117\n",
      "Iteration : 40 ; Train RMSE = 0.9471 ; Test RMSE = 1.0026\n",
      "Iteration : 50 ; Train RMSE = 0.9337 ; Test RMSE = 0.9968\n",
      "Iteration : 60 ; Train RMSE = 0.9234 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9151 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9082 ; Test RMSE = 0.9887\n",
      "Iteration : 90 ; Train RMSE = 0.9023 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8973 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8928 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8888 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8852 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8818 ; Test RMSE = 0.9847\n",
      "Iteration : 150 ; Train RMSE = 0.8787 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8758 ; Test RMSE = 0.9845\n",
      "Iteration : 170 ; Train RMSE = 0.8730 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8702 ; Test RMSE = 0.9845\n",
      "Iteration : 190 ; Train RMSE = 0.8675 ; Test RMSE = 0.9846\n",
      "Iteration : 200 ; Train RMSE = 0.8648 ; Test RMSE = 0.9847\n",
      "Iteration : 210 ; Train RMSE = 0.8621 ; Test RMSE = 0.9849\n",
      "Iteration : 220 ; Train RMSE = 0.8593 ; Test RMSE = 0.9851\n",
      "Iteration : 230 ; Train RMSE = 0.8563 ; Test RMSE = 0.9852\n",
      "Iteration : 240 ; Train RMSE = 0.8531 ; Test RMSE = 0.9854\n",
      "Iteration : 250 ; Train RMSE = 0.8497 ; Test RMSE = 0.9857\n",
      "Iteration : 260 ; Train RMSE = 0.8460 ; Test RMSE = 0.9859\n",
      "Iteration : 270 ; Train RMSE = 0.8420 ; Test RMSE = 0.9861\n",
      "Iteration : 280 ; Train RMSE = 0.8377 ; Test RMSE = 0.9863\n",
      "Iteration : 290 ; Train RMSE = 0.8329 ; Test RMSE = 0.9865\n",
      "Iteration : 300 ; Train RMSE = 0.8277 ; Test RMSE = 0.9868\n",
      "K =  140\n",
      "Iteration : 10 ; Train RMSE = 1.0362 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9921 ; Test RMSE = 1.0272\n",
      "Iteration : 30 ; Train RMSE = 0.9653 ; Test RMSE = 1.0116\n",
      "Iteration : 40 ; Train RMSE = 0.9471 ; Test RMSE = 1.0026\n",
      "Iteration : 50 ; Train RMSE = 0.9338 ; Test RMSE = 0.9968\n",
      "Iteration : 60 ; Train RMSE = 0.9235 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9152 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9083 ; Test RMSE = 0.9886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 90 ; Train RMSE = 0.9025 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8974 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8929 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8890 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8854 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8821 ; Test RMSE = 0.9847\n",
      "Iteration : 150 ; Train RMSE = 0.8790 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8761 ; Test RMSE = 0.9845\n",
      "Iteration : 170 ; Train RMSE = 0.8733 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8706 ; Test RMSE = 0.9845\n",
      "Iteration : 190 ; Train RMSE = 0.8680 ; Test RMSE = 0.9846\n",
      "Iteration : 200 ; Train RMSE = 0.8654 ; Test RMSE = 0.9847\n",
      "Iteration : 210 ; Train RMSE = 0.8627 ; Test RMSE = 0.9849\n",
      "Iteration : 220 ; Train RMSE = 0.8600 ; Test RMSE = 0.9850\n",
      "Iteration : 230 ; Train RMSE = 0.8571 ; Test RMSE = 0.9852\n",
      "Iteration : 240 ; Train RMSE = 0.8540 ; Test RMSE = 0.9854\n",
      "Iteration : 250 ; Train RMSE = 0.8508 ; Test RMSE = 0.9856\n",
      "Iteration : 260 ; Train RMSE = 0.8473 ; Test RMSE = 0.9858\n",
      "Iteration : 270 ; Train RMSE = 0.8434 ; Test RMSE = 0.9861\n",
      "Iteration : 280 ; Train RMSE = 0.8393 ; Test RMSE = 0.9863\n",
      "Iteration : 290 ; Train RMSE = 0.8347 ; Test RMSE = 0.9865\n",
      "Iteration : 300 ; Train RMSE = 0.8297 ; Test RMSE = 0.9867\n",
      "K =  150\n",
      "Iteration : 10 ; Train RMSE = 1.0362 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9921 ; Test RMSE = 1.0272\n",
      "Iteration : 30 ; Train RMSE = 0.9653 ; Test RMSE = 1.0116\n",
      "Iteration : 40 ; Train RMSE = 0.9472 ; Test RMSE = 1.0025\n",
      "Iteration : 50 ; Train RMSE = 0.9338 ; Test RMSE = 0.9968\n",
      "Iteration : 60 ; Train RMSE = 0.9235 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9153 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9084 ; Test RMSE = 0.9886\n",
      "Iteration : 90 ; Train RMSE = 0.9026 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8975 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8931 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8891 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8855 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8823 ; Test RMSE = 0.9847\n",
      "Iteration : 150 ; Train RMSE = 0.8792 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8763 ; Test RMSE = 0.9845\n",
      "Iteration : 170 ; Train RMSE = 0.8736 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8710 ; Test RMSE = 0.9845\n",
      "Iteration : 190 ; Train RMSE = 0.8684 ; Test RMSE = 0.9846\n",
      "Iteration : 200 ; Train RMSE = 0.8658 ; Test RMSE = 0.9847\n",
      "Iteration : 210 ; Train RMSE = 0.8632 ; Test RMSE = 0.9849\n",
      "Iteration : 220 ; Train RMSE = 0.8605 ; Test RMSE = 0.9851\n",
      "Iteration : 230 ; Train RMSE = 0.8577 ; Test RMSE = 0.9853\n",
      "Iteration : 240 ; Train RMSE = 0.8548 ; Test RMSE = 0.9854\n",
      "Iteration : 250 ; Train RMSE = 0.8516 ; Test RMSE = 0.9857\n",
      "Iteration : 260 ; Train RMSE = 0.8482 ; Test RMSE = 0.9859\n",
      "Iteration : 270 ; Train RMSE = 0.8445 ; Test RMSE = 0.9861\n",
      "Iteration : 280 ; Train RMSE = 0.8405 ; Test RMSE = 0.9863\n",
      "Iteration : 290 ; Train RMSE = 0.8361 ; Test RMSE = 0.9865\n",
      "Iteration : 300 ; Train RMSE = 0.8314 ; Test RMSE = 0.9867\n",
      "K =  160\n",
      "Iteration : 10 ; Train RMSE = 1.0362 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9921 ; Test RMSE = 1.0272\n",
      "Iteration : 30 ; Train RMSE = 0.9654 ; Test RMSE = 1.0116\n",
      "Iteration : 40 ; Train RMSE = 0.9472 ; Test RMSE = 1.0025\n",
      "Iteration : 50 ; Train RMSE = 0.9339 ; Test RMSE = 0.9968\n",
      "Iteration : 60 ; Train RMSE = 0.9236 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9153 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9085 ; Test RMSE = 0.9887\n",
      "Iteration : 90 ; Train RMSE = 0.9027 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8976 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8932 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8893 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8857 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8825 ; Test RMSE = 0.9847\n",
      "Iteration : 150 ; Train RMSE = 0.8795 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8766 ; Test RMSE = 0.9845\n",
      "Iteration : 170 ; Train RMSE = 0.8740 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8714 ; Test RMSE = 0.9845\n",
      "Iteration : 190 ; Train RMSE = 0.8689 ; Test RMSE = 0.9846\n",
      "Iteration : 200 ; Train RMSE = 0.8664 ; Test RMSE = 0.9848\n",
      "Iteration : 210 ; Train RMSE = 0.8639 ; Test RMSE = 0.9849\n",
      "Iteration : 220 ; Train RMSE = 0.8613 ; Test RMSE = 0.9851\n",
      "Iteration : 230 ; Train RMSE = 0.8586 ; Test RMSE = 0.9853\n",
      "Iteration : 240 ; Train RMSE = 0.8558 ; Test RMSE = 0.9855\n",
      "Iteration : 250 ; Train RMSE = 0.8528 ; Test RMSE = 0.9857\n",
      "Iteration : 260 ; Train RMSE = 0.8496 ; Test RMSE = 0.9859\n",
      "Iteration : 270 ; Train RMSE = 0.8462 ; Test RMSE = 0.9861\n",
      "Iteration : 280 ; Train RMSE = 0.8424 ; Test RMSE = 0.9864\n",
      "Iteration : 290 ; Train RMSE = 0.8382 ; Test RMSE = 0.9866\n",
      "Iteration : 300 ; Train RMSE = 0.8337 ; Test RMSE = 0.9868\n",
      "K =  170\n",
      "Iteration : 10 ; Train RMSE = 1.0362 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9921 ; Test RMSE = 1.0272\n",
      "Iteration : 30 ; Train RMSE = 0.9654 ; Test RMSE = 1.0116\n",
      "Iteration : 40 ; Train RMSE = 0.9472 ; Test RMSE = 1.0025\n",
      "Iteration : 50 ; Train RMSE = 0.9339 ; Test RMSE = 0.9968\n",
      "Iteration : 60 ; Train RMSE = 0.9236 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9154 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9085 ; Test RMSE = 0.9886\n",
      "Iteration : 90 ; Train RMSE = 0.9027 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8977 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8933 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8893 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8858 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8826 ; Test RMSE = 0.9847\n",
      "Iteration : 150 ; Train RMSE = 0.8796 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8767 ; Test RMSE = 0.9845\n",
      "Iteration : 170 ; Train RMSE = 0.8741 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8715 ; Test RMSE = 0.9845\n",
      "Iteration : 190 ; Train RMSE = 0.8690 ; Test RMSE = 0.9846\n",
      "Iteration : 200 ; Train RMSE = 0.8665 ; Test RMSE = 0.9847\n",
      "Iteration : 210 ; Train RMSE = 0.8640 ; Test RMSE = 0.9849\n",
      "Iteration : 220 ; Train RMSE = 0.8615 ; Test RMSE = 0.9851\n",
      "Iteration : 230 ; Train RMSE = 0.8588 ; Test RMSE = 0.9852\n",
      "Iteration : 240 ; Train RMSE = 0.8561 ; Test RMSE = 0.9855\n",
      "Iteration : 250 ; Train RMSE = 0.8531 ; Test RMSE = 0.9857\n",
      "Iteration : 260 ; Train RMSE = 0.8500 ; Test RMSE = 0.9859\n",
      "Iteration : 270 ; Train RMSE = 0.8465 ; Test RMSE = 0.9861\n",
      "Iteration : 280 ; Train RMSE = 0.8428 ; Test RMSE = 0.9863\n",
      "Iteration : 290 ; Train RMSE = 0.8387 ; Test RMSE = 0.9865\n",
      "Iteration : 300 ; Train RMSE = 0.8343 ; Test RMSE = 0.9868\n",
      "K =  180\n",
      "Iteration : 10 ; Train RMSE = 1.0362 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9921 ; Test RMSE = 1.0272\n",
      "Iteration : 30 ; Train RMSE = 0.9654 ; Test RMSE = 1.0116\n",
      "Iteration : 40 ; Train RMSE = 0.9473 ; Test RMSE = 1.0025\n",
      "Iteration : 50 ; Train RMSE = 0.9339 ; Test RMSE = 0.9968\n",
      "Iteration : 60 ; Train RMSE = 0.9237 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9154 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9086 ; Test RMSE = 0.9887\n",
      "Iteration : 90 ; Train RMSE = 0.9028 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8978 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8934 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8895 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8859 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8827 ; Test RMSE = 0.9847\n",
      "Iteration : 150 ; Train RMSE = 0.8798 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8770 ; Test RMSE = 0.9845\n",
      "Iteration : 170 ; Train RMSE = 0.8744 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8719 ; Test RMSE = 0.9845\n",
      "Iteration : 190 ; Train RMSE = 0.8694 ; Test RMSE = 0.9846\n",
      "Iteration : 200 ; Train RMSE = 0.8670 ; Test RMSE = 0.9848\n",
      "Iteration : 210 ; Train RMSE = 0.8646 ; Test RMSE = 0.9849\n",
      "Iteration : 220 ; Train RMSE = 0.8622 ; Test RMSE = 0.9851\n",
      "Iteration : 230 ; Train RMSE = 0.8597 ; Test RMSE = 0.9853\n",
      "Iteration : 240 ; Train RMSE = 0.8570 ; Test RMSE = 0.9855\n",
      "Iteration : 250 ; Train RMSE = 0.8543 ; Test RMSE = 0.9857\n",
      "Iteration : 260 ; Train RMSE = 0.8513 ; Test RMSE = 0.9859\n",
      "Iteration : 270 ; Train RMSE = 0.8481 ; Test RMSE = 0.9862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 280 ; Train RMSE = 0.8446 ; Test RMSE = 0.9864\n",
      "Iteration : 290 ; Train RMSE = 0.8407 ; Test RMSE = 0.9866\n",
      "Iteration : 300 ; Train RMSE = 0.8365 ; Test RMSE = 0.9869\n",
      "K =  190\n",
      "Iteration : 10 ; Train RMSE = 1.0362 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9921 ; Test RMSE = 1.0272\n",
      "Iteration : 30 ; Train RMSE = 0.9654 ; Test RMSE = 1.0116\n",
      "Iteration : 40 ; Train RMSE = 0.9473 ; Test RMSE = 1.0025\n",
      "Iteration : 50 ; Train RMSE = 0.9340 ; Test RMSE = 0.9968\n",
      "Iteration : 60 ; Train RMSE = 0.9237 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9154 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9086 ; Test RMSE = 0.9887\n",
      "Iteration : 90 ; Train RMSE = 0.9028 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8978 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8934 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8895 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8860 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8828 ; Test RMSE = 0.9847\n",
      "Iteration : 150 ; Train RMSE = 0.8799 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8771 ; Test RMSE = 0.9845\n",
      "Iteration : 170 ; Train RMSE = 0.8745 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8720 ; Test RMSE = 0.9845\n",
      "Iteration : 190 ; Train RMSE = 0.8696 ; Test RMSE = 0.9846\n",
      "Iteration : 200 ; Train RMSE = 0.8672 ; Test RMSE = 0.9848\n",
      "Iteration : 210 ; Train RMSE = 0.8648 ; Test RMSE = 0.9849\n",
      "Iteration : 220 ; Train RMSE = 0.8624 ; Test RMSE = 0.9851\n",
      "Iteration : 230 ; Train RMSE = 0.8600 ; Test RMSE = 0.9853\n",
      "Iteration : 240 ; Train RMSE = 0.8574 ; Test RMSE = 0.9855\n",
      "Iteration : 250 ; Train RMSE = 0.8546 ; Test RMSE = 0.9857\n",
      "Iteration : 260 ; Train RMSE = 0.8517 ; Test RMSE = 0.9859\n",
      "Iteration : 270 ; Train RMSE = 0.8485 ; Test RMSE = 0.9861\n",
      "Iteration : 280 ; Train RMSE = 0.8451 ; Test RMSE = 0.9864\n",
      "Iteration : 290 ; Train RMSE = 0.8413 ; Test RMSE = 0.9866\n",
      "Iteration : 300 ; Train RMSE = 0.8372 ; Test RMSE = 0.9868\n",
      "K =  200\n",
      "Iteration : 10 ; Train RMSE = 1.0362 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9921 ; Test RMSE = 1.0272\n",
      "Iteration : 30 ; Train RMSE = 0.9654 ; Test RMSE = 1.0116\n",
      "Iteration : 40 ; Train RMSE = 0.9473 ; Test RMSE = 1.0026\n",
      "Iteration : 50 ; Train RMSE = 0.9340 ; Test RMSE = 0.9968\n",
      "Iteration : 60 ; Train RMSE = 0.9237 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9155 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9087 ; Test RMSE = 0.9886\n",
      "Iteration : 90 ; Train RMSE = 0.9029 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8979 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8935 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8896 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8861 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8829 ; Test RMSE = 0.9847\n",
      "Iteration : 150 ; Train RMSE = 0.8800 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8773 ; Test RMSE = 0.9845\n",
      "Iteration : 170 ; Train RMSE = 0.8747 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8722 ; Test RMSE = 0.9845\n",
      "Iteration : 190 ; Train RMSE = 0.8698 ; Test RMSE = 0.9846\n",
      "Iteration : 200 ; Train RMSE = 0.8675 ; Test RMSE = 0.9848\n",
      "Iteration : 210 ; Train RMSE = 0.8652 ; Test RMSE = 0.9849\n",
      "Iteration : 220 ; Train RMSE = 0.8628 ; Test RMSE = 0.9851\n",
      "Iteration : 230 ; Train RMSE = 0.8604 ; Test RMSE = 0.9853\n",
      "Iteration : 240 ; Train RMSE = 0.8579 ; Test RMSE = 0.9855\n",
      "Iteration : 250 ; Train RMSE = 0.8552 ; Test RMSE = 0.9857\n",
      "Iteration : 260 ; Train RMSE = 0.8524 ; Test RMSE = 0.9859\n",
      "Iteration : 270 ; Train RMSE = 0.8493 ; Test RMSE = 0.9862\n",
      "Iteration : 280 ; Train RMSE = 0.8460 ; Test RMSE = 0.9864\n",
      "Iteration : 290 ; Train RMSE = 0.8424 ; Test RMSE = 0.9866\n",
      "Iteration : 300 ; Train RMSE = 0.8384 ; Test RMSE = 0.9869\n",
      "K =  210\n",
      "Iteration : 10 ; Train RMSE = 1.0362 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9922 ; Test RMSE = 1.0272\n",
      "Iteration : 30 ; Train RMSE = 0.9655 ; Test RMSE = 1.0116\n",
      "Iteration : 40 ; Train RMSE = 0.9473 ; Test RMSE = 1.0025\n",
      "Iteration : 50 ; Train RMSE = 0.9340 ; Test RMSE = 0.9968\n",
      "Iteration : 60 ; Train RMSE = 0.9238 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9155 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9087 ; Test RMSE = 0.9887\n",
      "Iteration : 90 ; Train RMSE = 0.9029 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8980 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8936 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8897 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8862 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8830 ; Test RMSE = 0.9847\n",
      "Iteration : 150 ; Train RMSE = 0.8801 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8774 ; Test RMSE = 0.9845\n",
      "Iteration : 170 ; Train RMSE = 0.8748 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8724 ; Test RMSE = 0.9845\n",
      "Iteration : 190 ; Train RMSE = 0.8700 ; Test RMSE = 0.9846\n",
      "Iteration : 200 ; Train RMSE = 0.8677 ; Test RMSE = 0.9848\n",
      "Iteration : 210 ; Train RMSE = 0.8654 ; Test RMSE = 0.9849\n",
      "Iteration : 220 ; Train RMSE = 0.8631 ; Test RMSE = 0.9851\n",
      "Iteration : 230 ; Train RMSE = 0.8607 ; Test RMSE = 0.9853\n",
      "Iteration : 240 ; Train RMSE = 0.8582 ; Test RMSE = 0.9855\n",
      "Iteration : 250 ; Train RMSE = 0.8556 ; Test RMSE = 0.9857\n",
      "Iteration : 260 ; Train RMSE = 0.8528 ; Test RMSE = 0.9859\n",
      "Iteration : 270 ; Train RMSE = 0.8499 ; Test RMSE = 0.9861\n",
      "Iteration : 280 ; Train RMSE = 0.8466 ; Test RMSE = 0.9864\n",
      "Iteration : 290 ; Train RMSE = 0.8431 ; Test RMSE = 0.9866\n",
      "Iteration : 300 ; Train RMSE = 0.8392 ; Test RMSE = 0.9868\n",
      "K =  220\n",
      "Iteration : 10 ; Train RMSE = 1.0362 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9922 ; Test RMSE = 1.0272\n",
      "Iteration : 30 ; Train RMSE = 0.9655 ; Test RMSE = 1.0116\n",
      "Iteration : 40 ; Train RMSE = 0.9473 ; Test RMSE = 1.0025\n",
      "Iteration : 50 ; Train RMSE = 0.9341 ; Test RMSE = 0.9968\n",
      "Iteration : 60 ; Train RMSE = 0.9238 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9156 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9087 ; Test RMSE = 0.9886\n",
      "Iteration : 90 ; Train RMSE = 0.9030 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8980 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8937 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8898 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8863 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8831 ; Test RMSE = 0.9847\n",
      "Iteration : 150 ; Train RMSE = 0.8802 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8775 ; Test RMSE = 0.9845\n",
      "Iteration : 170 ; Train RMSE = 0.8750 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8726 ; Test RMSE = 0.9845\n",
      "Iteration : 190 ; Train RMSE = 0.8702 ; Test RMSE = 0.9846\n",
      "Iteration : 200 ; Train RMSE = 0.8680 ; Test RMSE = 0.9848\n",
      "Iteration : 210 ; Train RMSE = 0.8657 ; Test RMSE = 0.9849\n",
      "Iteration : 220 ; Train RMSE = 0.8634 ; Test RMSE = 0.9851\n",
      "Iteration : 230 ; Train RMSE = 0.8611 ; Test RMSE = 0.9853\n",
      "Iteration : 240 ; Train RMSE = 0.8587 ; Test RMSE = 0.9855\n",
      "Iteration : 250 ; Train RMSE = 0.8562 ; Test RMSE = 0.9857\n",
      "Iteration : 260 ; Train RMSE = 0.8535 ; Test RMSE = 0.9859\n",
      "Iteration : 270 ; Train RMSE = 0.8506 ; Test RMSE = 0.9862\n",
      "Iteration : 280 ; Train RMSE = 0.8475 ; Test RMSE = 0.9864\n",
      "Iteration : 290 ; Train RMSE = 0.8440 ; Test RMSE = 0.9866\n",
      "Iteration : 300 ; Train RMSE = 0.8403 ; Test RMSE = 0.9868\n",
      "K =  230\n",
      "Iteration : 10 ; Train RMSE = 1.0362 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9922 ; Test RMSE = 1.0272\n",
      "Iteration : 30 ; Train RMSE = 0.9655 ; Test RMSE = 1.0116\n",
      "Iteration : 40 ; Train RMSE = 0.9474 ; Test RMSE = 1.0026\n",
      "Iteration : 50 ; Train RMSE = 0.9341 ; Test RMSE = 0.9968\n",
      "Iteration : 60 ; Train RMSE = 0.9238 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9156 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9088 ; Test RMSE = 0.9887\n",
      "Iteration : 90 ; Train RMSE = 0.9030 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8981 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8937 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8899 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8864 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8832 ; Test RMSE = 0.9847\n",
      "Iteration : 150 ; Train RMSE = 0.8803 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8777 ; Test RMSE = 0.9845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 170 ; Train RMSE = 0.8751 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8728 ; Test RMSE = 0.9846\n",
      "Iteration : 190 ; Train RMSE = 0.8705 ; Test RMSE = 0.9847\n",
      "Iteration : 200 ; Train RMSE = 0.8682 ; Test RMSE = 0.9848\n",
      "Iteration : 210 ; Train RMSE = 0.8660 ; Test RMSE = 0.9849\n",
      "Iteration : 220 ; Train RMSE = 0.8638 ; Test RMSE = 0.9851\n",
      "Iteration : 230 ; Train RMSE = 0.8615 ; Test RMSE = 0.9853\n",
      "Iteration : 240 ; Train RMSE = 0.8592 ; Test RMSE = 0.9855\n",
      "Iteration : 250 ; Train RMSE = 0.8568 ; Test RMSE = 0.9857\n",
      "Iteration : 260 ; Train RMSE = 0.8542 ; Test RMSE = 0.9860\n",
      "Iteration : 270 ; Train RMSE = 0.8514 ; Test RMSE = 0.9862\n",
      "Iteration : 280 ; Train RMSE = 0.8484 ; Test RMSE = 0.9864\n",
      "Iteration : 290 ; Train RMSE = 0.8451 ; Test RMSE = 0.9867\n",
      "Iteration : 300 ; Train RMSE = 0.8415 ; Test RMSE = 0.9869\n",
      "K =  240\n",
      "Iteration : 10 ; Train RMSE = 1.0362 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9922 ; Test RMSE = 1.0272\n",
      "Iteration : 30 ; Train RMSE = 0.9655 ; Test RMSE = 1.0116\n",
      "Iteration : 40 ; Train RMSE = 0.9474 ; Test RMSE = 1.0025\n",
      "Iteration : 50 ; Train RMSE = 0.9341 ; Test RMSE = 0.9968\n",
      "Iteration : 60 ; Train RMSE = 0.9238 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9156 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9088 ; Test RMSE = 0.9887\n",
      "Iteration : 90 ; Train RMSE = 0.9031 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8981 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8938 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8899 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8864 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8833 ; Test RMSE = 0.9847\n",
      "Iteration : 150 ; Train RMSE = 0.8804 ; Test RMSE = 0.9846\n",
      "Iteration : 160 ; Train RMSE = 0.8777 ; Test RMSE = 0.9845\n",
      "Iteration : 170 ; Train RMSE = 0.8752 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8728 ; Test RMSE = 0.9846\n",
      "Iteration : 190 ; Train RMSE = 0.8706 ; Test RMSE = 0.9847\n",
      "Iteration : 200 ; Train RMSE = 0.8683 ; Test RMSE = 0.9848\n",
      "Iteration : 210 ; Train RMSE = 0.8661 ; Test RMSE = 0.9849\n",
      "Iteration : 220 ; Train RMSE = 0.8639 ; Test RMSE = 0.9851\n",
      "Iteration : 230 ; Train RMSE = 0.8617 ; Test RMSE = 0.9853\n",
      "Iteration : 240 ; Train RMSE = 0.8594 ; Test RMSE = 0.9855\n",
      "Iteration : 250 ; Train RMSE = 0.8569 ; Test RMSE = 0.9857\n",
      "Iteration : 260 ; Train RMSE = 0.8544 ; Test RMSE = 0.9860\n",
      "Iteration : 270 ; Train RMSE = 0.8516 ; Test RMSE = 0.9862\n",
      "Iteration : 280 ; Train RMSE = 0.8487 ; Test RMSE = 0.9864\n",
      "Iteration : 290 ; Train RMSE = 0.8454 ; Test RMSE = 0.9867\n",
      "Iteration : 300 ; Train RMSE = 0.8419 ; Test RMSE = 0.9869\n",
      "K =  250\n",
      "Iteration : 10 ; Train RMSE = 1.0362 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9922 ; Test RMSE = 1.0272\n",
      "Iteration : 30 ; Train RMSE = 0.9655 ; Test RMSE = 1.0116\n",
      "Iteration : 40 ; Train RMSE = 0.9474 ; Test RMSE = 1.0025\n",
      "Iteration : 50 ; Train RMSE = 0.9341 ; Test RMSE = 0.9968\n",
      "Iteration : 60 ; Train RMSE = 0.9239 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9156 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9088 ; Test RMSE = 0.9886\n",
      "Iteration : 90 ; Train RMSE = 0.9031 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8981 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8938 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8899 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8865 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8833 ; Test RMSE = 0.9847\n",
      "Iteration : 150 ; Train RMSE = 0.8805 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8778 ; Test RMSE = 0.9845\n",
      "Iteration : 170 ; Train RMSE = 0.8753 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8729 ; Test RMSE = 0.9846\n",
      "Iteration : 190 ; Train RMSE = 0.8706 ; Test RMSE = 0.9846\n",
      "Iteration : 200 ; Train RMSE = 0.8684 ; Test RMSE = 0.9848\n",
      "Iteration : 210 ; Train RMSE = 0.8662 ; Test RMSE = 0.9849\n",
      "Iteration : 220 ; Train RMSE = 0.8640 ; Test RMSE = 0.9851\n",
      "Iteration : 230 ; Train RMSE = 0.8618 ; Test RMSE = 0.9853\n",
      "Iteration : 240 ; Train RMSE = 0.8595 ; Test RMSE = 0.9855\n",
      "Iteration : 250 ; Train RMSE = 0.8571 ; Test RMSE = 0.9857\n",
      "Iteration : 260 ; Train RMSE = 0.8545 ; Test RMSE = 0.9859\n",
      "Iteration : 270 ; Train RMSE = 0.8518 ; Test RMSE = 0.9862\n",
      "Iteration : 280 ; Train RMSE = 0.8489 ; Test RMSE = 0.9864\n",
      "Iteration : 290 ; Train RMSE = 0.8457 ; Test RMSE = 0.9866\n",
      "Iteration : 300 ; Train RMSE = 0.8422 ; Test RMSE = 0.9869\n",
      "K =  260\n",
      "Iteration : 10 ; Train RMSE = 1.0362 ; Test RMSE = 1.0571\n",
      "Iteration : 20 ; Train RMSE = 0.9922 ; Test RMSE = 1.0272\n",
      "Iteration : 30 ; Train RMSE = 0.9655 ; Test RMSE = 1.0116\n",
      "Iteration : 40 ; Train RMSE = 0.9474 ; Test RMSE = 1.0026\n",
      "Iteration : 50 ; Train RMSE = 0.9341 ; Test RMSE = 0.9968\n",
      "Iteration : 60 ; Train RMSE = 0.9239 ; Test RMSE = 0.9931\n",
      "Iteration : 70 ; Train RMSE = 0.9157 ; Test RMSE = 0.9905\n",
      "Iteration : 80 ; Train RMSE = 0.9089 ; Test RMSE = 0.9886\n",
      "Iteration : 90 ; Train RMSE = 0.9031 ; Test RMSE = 0.9873\n",
      "Iteration : 100 ; Train RMSE = 0.8982 ; Test RMSE = 0.9864\n",
      "Iteration : 110 ; Train RMSE = 0.8938 ; Test RMSE = 0.9857\n",
      "Iteration : 120 ; Train RMSE = 0.8900 ; Test RMSE = 0.9852\n",
      "Iteration : 130 ; Train RMSE = 0.8865 ; Test RMSE = 0.9849\n",
      "Iteration : 140 ; Train RMSE = 0.8834 ; Test RMSE = 0.9847\n",
      "Iteration : 150 ; Train RMSE = 0.8806 ; Test RMSE = 0.9845\n",
      "Iteration : 160 ; Train RMSE = 0.8779 ; Test RMSE = 0.9845\n",
      "Iteration : 170 ; Train RMSE = 0.8754 ; Test RMSE = 0.9845\n",
      "Iteration : 180 ; Train RMSE = 0.8731 ; Test RMSE = 0.9846\n",
      "Iteration : 190 ; Train RMSE = 0.8708 ; Test RMSE = 0.9847\n",
      "Iteration : 200 ; Train RMSE = 0.8686 ; Test RMSE = 0.9848\n",
      "Iteration : 210 ; Train RMSE = 0.8664 ; Test RMSE = 0.9849\n",
      "Iteration : 220 ; Train RMSE = 0.8643 ; Test RMSE = 0.9851\n",
      "Iteration : 230 ; Train RMSE = 0.8621 ; Test RMSE = 0.9853\n",
      "Iteration : 240 ; Train RMSE = 0.8598 ; Test RMSE = 0.9855\n",
      "Iteration : 250 ; Train RMSE = 0.8575 ; Test RMSE = 0.9857\n",
      "Iteration : 260 ; Train RMSE = 0.8550 ; Test RMSE = 0.9860\n",
      "Iteration : 270 ; Train RMSE = 0.8524 ; Test RMSE = 0.9862\n",
      "Iteration : 280 ; Train RMSE = 0.8495 ; Test RMSE = 0.9864\n",
      "Iteration : 290 ; Train RMSE = 0.8464 ; Test RMSE = 0.9867\n",
      "Iteration : 300 ; Train RMSE = 0.8430 ; Test RMSE = 0.9869\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "index = []\n",
    "\n",
    "for K in range(50, 261, 10):\n",
    "    print('K = ', K)\n",
    "    R_temp = ratings.pivot(index='user_id', columns='movie_id', values='rating').fillna(0)\n",
    "    mf = NEW_MF(R_temp, K=K, alpha=0.001, beta=0.02, iterations=300, verbose=True)\n",
    "    test_set = mf.set_test(ratings_test)\n",
    "    result = mf.test()\n",
    "    \n",
    "    # k를 담는 리트\n",
    "    index.append(K)\n",
    "    # iteration, train_rmse, test_rmse를 담는 리스트\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54a2c397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적의 iterations 찾기\n",
    "summary = []\n",
    "for i in range(len(results)):\n",
    "    RMSE = []\n",
    "    for result in results[i]:\n",
    "        RMSE.append(result[2])\n",
    "    # 가장 낮은 RMSE를 찾고\n",
    "    min = np.min(RMSE)\n",
    "    # min의 index 즉, iteration 찾아서\n",
    "    j = RMSE.index(min)\n",
    "    # K, iteration 수, 최적의 RMSE 반환\n",
    "    summary.append([index[i], j+1, RMSE[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7a995ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프로 확인\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(index, [x[2] for x in summary])\n",
    "\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
